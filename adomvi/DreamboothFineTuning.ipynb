{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5554c36c-4e1b-478e-a8d5-b817171a0484",
   "metadata": {},
   "source": [
    "# Dreambooth fine-tuning for Stable Diffusion\n",
    "\n",
    "In this notebook, we'll try to use [Dreambooth](https://huggingface.co/docs/diffusers/training/dreambooth) to generate images of military vehicles for training our vehicle detection model. Our model suffers from a lack of diversity in its training data available, so we'll see if we can improve our training dataset with synthetic images generated by a text-to-image model. [Dreambooth](https://arxiv.org/abs/2208.12242) is a method to personnalize text-to-image models with just a few images of a subject.\n",
    "\n",
    "We use the training script provided by diffusers [here](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth) and follow the guide available [here](https://huggingface.co/docs/diffusers/training/dreambooth).\n",
    "\n",
    "## Configuration\n",
    "As explained in the guide, dreambooth is quite susceptible to overfitting and finding the right hyperparameters can be challenging. We tried various configurations to find the one that works best for our use case. We also followed the advice from [their analysis](https://huggingface.co/blog/dreambooth) on how to train dreambooth.\n",
    "\n",
    "Overall, we found that using Low-Rank Adaptation of Large Language Models (LoRA) gives better results, or at least is easier to train and finetune. We also took advantage of GPU optimization tools, such as [xFormers](https://github.com/facebookresearch/xformers) (enabled by adding the `--enable_xformers_memory_efficient_attention` argument to the training script) and [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) 8-bit optimizer (add `--use_8bit_adam`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ecb1a0-80ea-41c4-8f33-8f6925f25a71",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before running the script, we need to install the dependencies. Installing the dependencies with poetry caused some issues, so we recommend to use a python virtual env manager to install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b8534-2aa3-4660-bf46-3bee3aac727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the huggingface diffusers repo and install dependencies\n",
    "!git clone https://github.com/huggingface/diffusers ../diffusers\n",
    "!pip install -e ../diffusers\n",
    "!pip install -U -r ../diffusers/examples/dreambooth/requirements.txt\n",
    "!pip install bitsandbytes xformer\n",
    "!accelerate config default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdfb143-be9b-4c07-bd41-6a2a1769ebf5",
   "metadata": {},
   "source": [
    "## Fine-tune the model\n",
    "\n",
    "To train the model, we found these settings work best: we train with LoRA, on 800 steps, saving checkpoints every 250 step. We finetune the text_encoder along with the unet. The following script will also train dreambooth with prior-preservation by generating 100 images of tank. However, the images generated with Stable Diffusion often have undesirable artifacts and it would probably be better to use our own class dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41557679-a6f4-47f4-aea8-337fa43e49a9",
   "metadata": {},
   "source": [
    "### Instance data\n",
    "\n",
    "To train dreambooth, you need to provide a limited sample of a specific subject instance that dreambooth will learn. In this notebook, we'll try to use dreambooth to generate images of a specific type of tank, the [Leclerc Tank](https://en.wikipedia.org/wiki/Leclerc_tank). We created a dataset of 15 images of the Leclerc Tank to finetune dreambooth on by using our [GoogleImageScraper](./scraper/google.py). You can generate your own dataset or use the one we added to the [resources dir](../resources/leclerc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0340492c-c861-41db-83c4-0b105d799dce",
   "metadata": {},
   "source": [
    "### Prior preservation\n",
    "\n",
    "Prior preservation is used to avoid overfitting and language-drift (see the [dreambooth paper](https://arxiv.org/abs/2208.12242)). For prior preservation, we need other images of the same class (military vehicles). We have two options: we can use some of the tank images that we gathered from open-source image datasets (see the [TankDetectionYoloV8Train notebook](./TankDetectionYolov8Train.ipynb), or we can use Stable Diffusion to generate thoses images itself. Either case works fine, however the images generated with Stable Diffusion often have undesirable artifacts (multiple canons, etc.) which could probably be avoided by using a proper curated dataset.\n",
    "\n",
    "To sample 100 tank images from our `military-vehicles` dataset, run the following script. Otherwise, if the `tanks` directory is empty, Stable Diffusion will generate the missing tank images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89e94d-ac38-4ba9-879f-69221847628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "export_dir = \"../tanks\"\n",
    "\n",
    "# The dataset or view to export\n",
    "dataset = fo.load_dataset(\"military-vehicles\").take(100)\n",
    "\n",
    "# Export the dataset\n",
    "dataset.export(export_dir=export_dir, dataset_type=fo.types.ImageDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b71763-ce67-41a8-b007-4c12ee2f0572",
   "metadata": {},
   "source": [
    "To train the model, we found these settings work best: we train with LoRA, on 800 steps, saving checkpoints every 250 step. We finetune the text_encoder along with the unet. We use prior-preservation with 100 samples of tank images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d420336-0963-4ddc-a3a0-e890d907a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch ../diffusers/examples/dreambooth/train_dreambooth_lora.py \\\n",
    "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\"  \\\n",
    "  --train_text_encoder \\\n",
    "  --instance_data_dir=\"../resources/leclerc\" \\\n",
    "  --class_data_dir=\"../tanks\" \\\n",
    "  --output_dir=\"../adomvi-dream-tank\" \\\n",
    "  --with_prior_preservation \\\n",
    "  --prior_loss_weight=1.0 \\\n",
    "  --instance_prompt=\"a photo of [L] tank\" \\\n",
    "  --class_prompt=\"a photo of a tank\" \\\n",
    "  --num_class_images=100 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --use_8bit_adam \\\n",
    "  --enable_xformers_memory_efficient_attention \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --checkpointing_steps=250 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=800 \\\n",
    "  --validation_prompt=\"A photo of [L] tank on the moon\" \\\n",
    "  --validation_epochs=50 \\\n",
    "  --seed=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8abbc1",
   "metadata": {},
   "source": [
    "### Running inference\n",
    "\n",
    "Once our model is trained, we can run inference to generate new images of a leclerc tank. Since our original military vehicle dataset lacks images taken from a distance, or where the subject is partially hidden, we try to generate such images by constructing a complex prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a9bed-feb4-4ef9-8601-70ac3f91264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# to load a checkpoint, change the lora_model_id path, i.e. lora_model_id = \"adomvi-dream-tank/checkpoint-500\"\n",
    "lora_model_id = \"../adomvi-dream-tank\"\n",
    "model_base = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_base, torch_dtype=torch.float16, safety_checker=None)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "pipe.load_lora_weights(lora_model_id)\n",
    "\n",
    "prompts = {\n",
    "    \"desert\": \"[L] tank moving in the desert behind rocks, as seen from the top of a hill far away, respecting scale and dimensions\",\n",
    "    \"snow\": \"[L] tank moving in a forest under snow, as seen from very far away, respecting scale and dimensions\",\n",
    "    \"rain\": \"[L] tank on a field under heavy rain, as seen from very far away, respecting scale and dimensions\",\n",
    "    \"battle\": \"[L] tank fighting on a battlefield, as seen from very far away, respecting scale and dimensions\",\n",
    "}\n",
    "\n",
    "negative_prompt = \"close-up, upfront, unobstructed, bad scale\"\n",
    "\n",
    "for token, prompt in prompts.items():\n",
    "    images = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        # width=512,\n",
    "        # height=768,\n",
    "        num_inference_steps=100,\n",
    "        num_images_per_prompt=4,\n",
    "        # generator=torch.manual_seed(0),\n",
    "        guidance_scale=7.5,\n",
    "        cross_attention_kwargs={\"scale\": 0.9},\n",
    "    ).images\n",
    "\n",
    "    for i in range(4):\n",
    "        images[i].save(f\"../inference/tank-{token}-{i}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
